{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with Convolutional Neural Networks\n",
    "\n",
    "Original code at: https://github.com/ageron/handson-ml/blob/master/13_convolutional_neural_networks.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the notebook compatible with both Python 2 and 3\n",
    "\n",
    "http://python-future.org/compatible_idioms.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot graphs inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "1.16.2\n",
      "3.0.3\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "print(matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and extract the MNIST libraries\n",
    "\n",
    "The original site where this dataset is available: http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Store the MNIST data in mnist_data/\n",
    "mnist = input_data.read_data_sets(\"mnist_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to display one digit image\n",
    "\n",
    "Reshape the data from 1-D array to a 2-D array of 28x28 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_digit(digit):\n",
    "    plt.imshow(digit.reshape(28, 28), cmap=\"Greys\", interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the training and test data and the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_digits, training_labels = mnist.train.next_batch(10000)\n",
    "test_digits, test_labels = mnist.test.next_batch(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_digit(training_digits[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 9, 8, 5, 6], dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensions of MNIST images\n",
    "\n",
    "* Each image is 28x28 pixels\n",
    "* The images are grayscale and have just one channel\n",
    "* The number of inputs is equal to the number of pixels in each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 28\n",
    "width = 28\n",
    "channels = 1\n",
    "n_inputs = height * width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a CNN with 2 convolutional layers and one max pool layer\n",
    "\n",
    "* Specify the number of **feature maps** in each layer, a feature map highlights that area in an image which is most similar to the filter applied\n",
    "* The kernel size indicates the **dimensions of the filter** which is applied to the image. The filter variables are created for you and initialized randomly\n",
    "* The stride is the steps by which the filter moves over the input, the **distance between two receptive fields on the input**\n",
    "* \"SAME\" padding indicates that the convolutional layer **uses zero padding** on the inputs and will consider all inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_feature_maps = 32\n",
    "conv1_kernel_size = 3\n",
    "conv1_stride = 1\n",
    "conv1_pad = \"SAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2_feature_maps = 64\n",
    "conv2_kernel_size = 3\n",
    "conv2_stride = 2\n",
    "conv2_pad = \"SAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool3_feature_maps = conv2_feature_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One fully connected layer\n",
    "\n",
    "* 64 neurons in the layer\n",
    "* 10 outputs corresponding to the digits 0-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fullyconn1 = 64\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders for training data and labels\n",
    "\n",
    "* The training dataset placeholder can have any number of instances and each instance is an array of n_inputs = 28x28 = 784 pixels\n",
    "* The images are fed to the convolutional layer as a 4D tensor *[batch_size, height, width, channels]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.int32, shape=[None], name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect the convolutional layers\n",
    "\n",
    "* Each layer uses the ReLU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-25-a6590d9f8110>:4: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_feature_maps,\n",
    "                         kernel_size=conv1_kernel_size,\n",
    "                         strides=conv1_stride, padding=conv1_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2 = tf.layers.conv2d(conv1, filters=conv2_feature_maps, \n",
    "                         kernel_size=conv2_kernel_size,\n",
    "                         strides=conv2_stride, padding=conv2_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect a max pooling layer\n",
    "\n",
    "* The filter is a 2x2 filter\n",
    "* The stride is 2 both horizontally and vertically\n",
    "* This results in an image that is **1/4th the size of the original image**\n",
    "* Reshape the pooled layer to be a **1-D vector (flatten it)**. It now has only 1/4th the number of pixels in each feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool3 = tf.nn.max_pool(conv2,\n",
    "                       ksize=[1, 2, 2, 1],\n",
    "                       strides=[1, 2, 2, 1],\n",
    "                       padding=\"VALID\")\n",
    "\n",
    "pool3_flat = tf.reshape(pool3, shape=[-1, pool3_feature_maps * 7 * 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-28-3501d0850430>:2: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "fullyconn1 = tf.layers.dense(pool3_flat, n_fullyconn1,\n",
    "                             activation=tf.nn.relu, name=\"fc1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The final output layer with softmax activation\n",
    "\n",
    "Do not apply the softmax activation to this layer. The *tf.nn.sparse_softmax_cross_entropy_with_logits* will apply the softmax activation as well as calculate the cross-entropy as our cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(fullyconn1, n_outputs, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy as a cost function\n",
    "\n",
    "* Use the Adam optimizer which in most cases performs better than the simple gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                          labels=y)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check correctness and accuracy of the prediction\n",
    "\n",
    "* Check whether the highest probability output in logits is equal to the y-label\n",
    "* Check the accuracy across all predictions (How many predictions did we get right?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with the training data, measure accuracy with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 1.0 Test accuracy: 0.9752\n",
      "1 Train accuracy: 0.99 Test accuracy: 0.9812\n",
      "2 Train accuracy: 0.99 Test accuracy: 0.9867\n",
      "3 Train accuracy: 1.0 Test accuracy: 0.9866\n",
      "4 Train accuracy: 0.99 Test accuracy: 0.9895\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "        save_path = saver.save(sess, \"./my_mnist_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
